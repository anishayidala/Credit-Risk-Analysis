# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g2DYhFMnnGUTAdFZ8XY5k5HgOuOY2CK7
"""

import pandas as pd
import numpy as np
from sklearn import  tree
from sklearn.ensemble import  RandomForestClassifier
from xgboost import XGBClassifier
import seaborn as sns
import matplotlib.pyplot as plt

#reading data from train file
traindata = pd.read_csv("./Train.csv")

#check for null values
traindata.isnull().sum()

traindata

#renaming columns for better understanding
traindata = traindata.rename(columns={'id':'id','F1': 'degreeyearscompletion', 'F2': 'hoursworkedperweek','F3': 'Relationship', 'F4': 'OccupationType','F5': 'Gains', 'F6': 'Loss','F7': 'MaritalStatus', 'F8': 'EmploymentType','F9': 'EducationType', 'F10': 'Race','F11': 'Male/Female', 'credit': 'creditscore' })

traindata

#finding correlation between the features
correlation = traindata.corr()

correlation

#representing correlation between the features in a heatmap using seaborn
plt.figure(figsize = (10,8))
sns.heatmap(correlation,annot = True)
plt.show()

#eliminating features from the traindata
traindata = traindata.drop(["id"],axis=1)
traindata = traindata.drop(["EducationType"],axis=1)
traindata= traindata.drop(["MaritalStatus"],axis=1)

traindata

testdata = pd.read_csv("./Test.csv")

testdata.isnull().sum()

#renaming columns for better understanding
testdata = testdata.rename(columns={'id':'id','F1': 'degreeyearscompletion', 'F2': 'hoursworkedperweek','F3': 'Relationship', 'F4': 'OccupationType','F5': 'Gains', 'F6': 'Loss','F7': 'MaritalStatus', 'F8': 'EmploymentType','F9': 'EducationType', 'F10': 'Race','F11': 'Male/Female'})

testdata

#eliminating features from the testdata
testdata = testdata.drop(["id"],axis=1)
testdata = testdata.drop(["EducationType"],axis=1)
testdata= testdata.drop(["MaritalStatus"],axis=1)

testdata

#normalized function to scale features
def normalized(A):
    A = (A- np.min(A))/ (np.max(A) - np.min(A))
    return A

#selecting continuous variables
continuous_values_train = traindata[['degreeyearscompletion','hoursworkedperweek','Gains','Loss']]

continuous_values_test = testdata[['degreeyearscompletion' , 'hoursworkedperweek','Gains','Loss']]

normalized_traindata = normalized(continuous_values_train)

normalized_testdata = normalized(continuous_values_test)

bin = [-0.1, 0.25, 0.50, 0.75, 1]

# continuous variables to categorical variables and onehot encoding
train_f1 =  pd.cut(normalized_traindata['degreeyearscompletion'],bins = bin)
train_f2 =  pd.cut(normalized_traindata['hoursworkedperweek'],bins = bin)
train_f5 =  pd.cut(normalized_traindata['Gains'],bins = bin)
train_f6 =  pd.cut(normalized_traindata['Loss'],bins = bin)

dummy_data_train =  pd.concat([train_f1, train_f2, train_f5, train_f6, traindata[['Race', 'Male/Female']]], axis =1)

test_f1 =  pd.cut(normalized_testdata['degreeyearscompletion'],bins = bin)
test_f2 =  pd.cut(normalized_testdata['hoursworkedperweek'],bins = bin)
test_f5 =  pd.cut(normalized_testdata['Gains'],bins = bin)
test_f6 =  pd.cut(normalized_testdata['Loss'],bins = bin)

dummy_data_test =  pd.concat([test_f1, test_f2, test_f5, test_f6, testdata[['Race', 'Male/Female']]], axis =1)

onehot_train = pd.get_dummies(dummy_data_train)

#splitting train data to fit to the model
X = pd.concat([traindata[['Relationship', 'OccupationType', 'EmploymentType']], onehot_train], axis =1)
y = traindata['creditscore']

onehot_test = pd.get_dummies(dummy_data_test)

#test data to make predictions on
Xtest = pd.concat([testdata[['Relationship', 'OccupationType', 'EmploymentType']], onehot_test], axis =1)

#random forest classifier
rfclassifier = RandomForestClassifier(n_estimators=500)

rfclasssifier_train = rfclassifier.fit(X,y)

output=rfclassifier.predict(Xtest)

output=output.reshape(output.size,1)

a_file = open("output.txt", "w")
for row in output:
    # np.savetxt(a_file, row)
    a_file.write(str(row[0])+"\n")

a_file.close()

#xgb classifier
model = XGBClassifier(scale_pos_weight=1,learning_rate=0.1,colsample_bytree = 0.6,subsample = 0.8,objective='binary:logistic',n_estimators=2000, reg_alpha = 0.5,max_depth=6, gamma=10)

model = XGBClassifier(scale_pos_weight=1,learning_rate=0.1,colsample_bytree = 0.6,subsample = 0.8,objective='binary:logistic',n_estimators=2000, reg_alpha = 0.5,max_depth=6, gamma=10)

# renaming columns to fit to xgb model
X= X.rename(columns={'degreeyearscompletion_(-0.1, 0.25]':'degreeyearscompletion1','degreeyearscompletion_(0.25, 0.5]':'degreeyearscompletion2','degreeyearscompletion_(0.5, 0.75]':'degreeyearscompletion3','degreeyearscompletion_(0.75, 1.0]':'degreeyearscompletion4','hoursworkedperweek_(-0.1, 0.25]':'hoursworkedperweek1','hoursworkedperweek_(0.25, 0.5]':'hoursworkedperweek2','hoursworkedperweek_(0.5, 0.75]':'hoursworkedperweek3','hoursworkedperweek_(0.75, 1.0]':'hoursworkedperweek4','Gains_(-0.1, 0.25]':'Gains1','Gains_(0.25, 0.5]':'Gains2','Gains_(0.5, 0.75]':'Gains3','Gains_(0.75, 1.0]':'Gains4','Loss_(-0.1, 0.25]':'Loss1','Loss_(0.25, 0.5]':'Loss2','Loss_(0.5, 0.75]':'Loss3','Loss_(0.75, 1.0]':'Loss4'})

Xtest = Xtest.rename(columns={'degreeyearscompletion_(-0.1, 0.25]':'degreeyearscompletion1','degreeyearscompletion_(0.25, 0.5]':'degreeyearscompletion2','degreeyearscompletion_(0.5, 0.75]':'degreeyearscompletion3','degreeyearscompletion_(0.75, 1.0]':'degreeyearscompletion4','hoursworkedperweek_(-0.1, 0.25]':'hoursworkedperweek1','hoursworkedperweek_(0.25, 0.5]':'hoursworkedperweek2','hoursworkedperweek_(0.5, 0.75]':'hoursworkedperweek3','hoursworkedperweek_(0.75, 1.0]':'hoursworkedperweek4','Gains_(-0.1, 0.25]':'Gains1','Gains_(0.25, 0.5]':'Gains2','Gains_(0.5, 0.75]':'Gains3','Gains_(0.75, 1.0]':'Gains4','Loss_(-0.1, 0.25]':'Loss1','Loss_(0.25, 0.5]':'Loss2','Loss_(0.5, 0.75]':'Loss3','Loss_(0.75, 1.0]':'Loss4'})

model.fit(X,y)

output=model.predict(Xtest)

xgbclassifier = pd.DataFrame(data=output, index=None)

xgbclassifier.to_csv('output1.txt',index=False,header=None)